{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Narichie/Large-Language-Model/blob/main/Data_Science_Profiles_%26_Interview_Preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiiLmHAdTOFE"
      },
      "source": [
        "## **Data Science Profiles and Interview Preparation Aided by a Large Language Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9NjJ16paneB"
      },
      "source": [
        "#### **Problem Definition**\n",
        "\n",
        "Designing Data Science Profiles and Interview Preparation Aided by an LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvRkP4VxhHoE"
      },
      "source": [
        "#### **BACKGROUND AND CONTEXT:**\n",
        "\n",
        "Opportunities for data scientists and engineers are set to rise as companies leverage abundant data and technologies like generative AI to enhance their competitive edge. This trend is evidenced by the projected growth of data science jobs in the U.S., which is expected to outpace the national average. However, competition is increasing due to the growing number of graduates entering the field, with many universities launching programs in Data Analysis, Data Science, and related areas. If you're preparing for an interview, consider the necessary skills and typical profiles in this expanding market."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dbMnpXjaneB"
      },
      "source": [
        "#### **Goal**\n",
        "\n",
        "**To create a connection to an LLM** for knowledge extraction using a Jupiter notebook and HuggingFace.\n",
        "\n",
        "**Prepare prompts to investigate:**\n",
        "* Specific steps to prepare for a data science interview.\n",
        "* The top five questions recruiters commonly ask entry-level data scientists.\n",
        "* Descriptive profiles for a set of three data scientists."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h0SehnXaneC"
      },
      "source": [
        "### **1. Prompt Engineering advanced techniques using Llama2-13B.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3dcI_ngTOmw"
      },
      "source": [
        "**Model Development and necessary installations and imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbNjMzBkSyAU",
        "outputId": "1febbfd9-8488-4c17-b3de-ec58c23f7b31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-cpp-python==0.2.28\n",
            "  Downloading llama_cpp_python-0.2.28.tar.gz (9.4 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/9.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/9.4 MB\u001b[0m \u001b[31m151.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m154.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python==0.2.28)\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python==0.2.28)\n",
            "  Downloading numpy-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python==0.2.28)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m151.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m167.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.28-cp310-cp310-linux_x86_64.whl size=8778637 sha256=11194c9ca52c614c6a8f5da3b090af9c892e19439d5ac6584e2525c3d2784569\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-s0hg1ddq/wheels/93/6e/a9/478cce089dc2a082bdcffe468a1c65465c91b25d911b30da82\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.28 numpy-2.2.0 typing-extensions-4.12.2\n"
          ]
        }
      ],
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.28 --force-reinstall --upgrade --no-cache-dir --verbose -q 2>/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKt9u7TxTPGd",
        "outputId": "f67bb795-fe7b-4c54-81d4-26ed66b29766"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/401.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub==0.23.2 -q 2>/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfhN1Yp8TPI6"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "1171bdd9341448aabf7e25dc9a89c3d9",
            "b311a251dc574718ae05bdf09daa0328",
            "b04197a7df584df39d815ba564d7f176",
            "a8d63d4c03284fdf9dd9dd19550a7ccb",
            "b247f6fb17d443589e24edffb019911a",
            "e3428d4c84ee4b5e9ed3720ba1d0251a",
            "785f28a2e22e478baafc9d655756f084",
            "3113afb0770b407a90984369c26427ce",
            "eccb130cd53f4ed28238688f8b0e847a",
            "19305114f6354e5c94c2920ffd9ef9c6",
            "97fa16ae5eb342779ee332799c19c631"
          ]
        },
        "id": "PFyrTOMQeMAx",
        "outputId": "81154c52-8fbd-403f-8a0c-d9c9a24cd386"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1171bdd9341448aabf7e25dc9a89c3d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "llama-2-13b-chat.Q5_K_M.gguf:   0%|          | 0.00/9.23G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        " # Model configuration\n",
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGUF\"\n",
        "model_basename = \"llama-2-13b-chat.Q5_K_M.gguf\"\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=model_name_or_path,\n",
        "    filename=model_basename\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqTdvrwhTPL5",
        "outputId": "2e128c7a-ddbf-4678-82ef-43ca8ffad0a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ],
      "source": [
        "    lcpp_llm = Llama(\n",
        "        model_path=model_path,\n",
        "        n_threads=2,\n",
        "        n_batch=512,\n",
        "        n_gpu_layers=43,\n",
        "        n_ctx=4096,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcWbcYwOaneF"
      },
      "outputs": [],
      "source": [
        "def generate_llama_response(user_prompt):\n",
        "\n",
        "    # System message\n",
        "    system_message = \"\"\"\n",
        "    [INST]<<SYS>> Respond to the user question based on the user prompt<</SYS>>[/INST]\n",
        "    \"\"\"\n",
        "\n",
        "    # Combine user_prompt and system_message to create the prompt\n",
        "    prompt = f\"{user_prompt}\\n{system_message}\"\n",
        "\n",
        "\n",
        "    # Generate a response from the LLaMA model\n",
        "    response = lcpp_llm(\n",
        "        prompt=prompt,\n",
        "        max_tokens=600,\n",
        "        temperature=0,\n",
        "        top_p=0.95,\n",
        "        repeat_penalty=1.2,\n",
        "        top_k=50,\n",
        "        stop=['INST'],\n",
        "        echo=False\n",
        "    )\n",
        "\n",
        "    # Extract and return the response text\n",
        "    response_text = response[\"choices\"][0][\"text\"]\n",
        "    return response_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfulR8goaneG"
      },
      "source": [
        "#### **Model Check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya-G4j6CaneG",
        "outputId": "08d79a66-92d6-4a5e-ba61-6e4f0795a7df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Sure, here are the top ten steps to prepare effectively for an entry-level data science interview:\n",
            "\n",
            "Step 1: Review Fundamentals - Brush up on your statistics, mathematics, and computer programming fundamentals. These concepts form the foundation of data science, and being comfortable with them is essential for success in an entry-level data science role.\n",
            "\n",
            "Step 2: Learn Programming Languages - Familiarize yourself with popular programming languages used in data science such as Python, R, SQL, and Julia. Practice writing code to solve problems related to data manipulation, analysis, and visualization.\n",
            "\n",
            "Step 3: Study Data Structures and Algorithms - Understand the time and space complexity of various data structures and algorithms commonly used in data science. This will help you write efficient and scalable code.\n",
            "\n",
            "Step 4: Learn Machine Learning Fundamentals - Familiarize yourself with supervised and unsupervised learning techniques, including linear regression, decision trees, clustering, etc. Practice implementing these algorithms using popular libraries like scikit-learn, TensorFlow, or PyTorch.\n",
            "\n",
            "Step 5: Work on Projects - Build a portfolio of projects that demonstrate your skills in data manipulation, analysis, visualization, and machine learning. This will help you showcase your abilities to potential employers.\n",
            "\n",
            "Step 6: Practice with Real-World Datasets - Practice solving real-world problems using publicly available datasets from sources like Kaggle, UCI Machine Learning Repository, or Data.gov. This will help you understand how to apply data science concepts in practical scenarios.\n",
            "\n",
            "Step 7: Learn Data Visualization Tools - Familiarize yourself with popular data visualization tools such as Tableau, Power BI, Matplotlib, Seaborn, etc. Practice creating interactive and informative visualizations that communicate insights effectively.\n",
            "\n",
            "Step 8: Study Big Data Technologies - Understand the basics of big data technologies like Hadoop, Spark, NoSQL databases, and cloud computing platforms like AWS or GCP. This will help you work with large datasets efficiently.\n",
            "\n",
            "Step 9: Learn Data Storytelling Techniques - Practice communicating insights effectively to non-technical stakeholders using storytelling techniques like narratives, metaphors, and visualizations.\n",
            "\n",
            "Step 10: Prepare for Common Interview Questions - Research common interview questions in data science and practice answering them confidently. This will help you feel more comfortable during the actual interview.\n",
            "\n",
            "By following these steps, you can prepare effectively for an entry-level data science interview and increase your chances of success.\n"
          ]
        }
      ],
      "source": [
        "# A. Data Science Interview Preparation\n",
        "# 1. Top 10 steps to prepare for a data science interview\n",
        "\n",
        "user_prompt = \"Provide the top ten steps to prepare effectively for an entry-level data science interview\"\n",
        "response_interview_steps = generate_llama_response(user_prompt)\n",
        "print(response_interview_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDH-zJyPaneG",
        "outputId": "3bdaa592-0c5d-4823-c126-aa1400c881c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Sure, here are the top five questions that a recruiter might ask an entry-level data scientist during an interview:\n",
            "\n",
            "1. Can you tell us about your experience with statistical analysis and modeling? How have you applied these skills in real-world projects or applications?\n",
            "\n",
            "This question is designed to assess the candidate's understanding of statistical concepts, their ability to apply them in practical situations, and their experience with data analysis software such as R, Python, or Excel. The recruiter wants to know if the candidate has hands-on experience working with datasets, performing statistical tests, and interpreting results.\n",
            "\n",
            "2. How do you approach data visualization? Can you show us an example of a project where you created visualizations that effectively communicated insights to stakeholders?\n",
            "\n",
            "This question is intended to evaluate the candidate's ability to communicate complex data insights to non-technical audiences through effective visualizations. The recruiter wants to see if the candidate has experience with popular data visualization tools such as Tableau, Power BI, or D3.js and can provide examples of how they have used these tools to create compelling visualizations that told a story.\n",
            "\n",
            "3. How do you stay current with new developments in machine learning and data science? Can you tell us about any recent articles, blogs, or research papers you've read?\n",
            "\n",
            "This question is designed to assess the candidate's level of enthusiasm for the field of data science, their ability to seek out new knowledge, and their willingness to continuously learn and improve. The recruiter wants to know if the candidate has a strong desire to stay up-to-date with the latest advancements in machine learning algorithms, deep learning techniques, or other areas of data science.\n",
            "\n",
            "4. Can you walk us through your experience working with large datasets? How do you manage and process big data sets?\n",
            "\n",
            "This question is intended to evaluate the candidate's ability to work with large amounts of data, their familiarity with distributed computing frameworks such as Hadoop or Spark, and their knowledge of data processing tools such as Apache Beam or AWS Glue. The recruiter wants to know if the candidate has experience working with petabyte-scale datasets, designing data pipelines, and optimizing data processing workflows.\n",
            "\n",
            "5. Can you give an example of a project where you had to work with a cross-functional team to solve a complex business problem? How did you collaborate with stakeholders to gather requirements and deliver insights?\n",
            "\n",
            "This question is designed to assess the candidate's ability to work effectively in teams, communicate with non-technical stakeholders, and deliver results that meet business objectives. The recruiter wants to know if the candidate has experience working collaboratively with data engineers, product\n"
          ]
        }
      ],
      "source": [
        "# 2. Top 5 questions recruiters commonly ask entry-level data scientists\n",
        "\n",
        "user_prompt = \"List the top five questions a recruiter might ask an entry-level data scientist during an interview\"\n",
        "response_recruiter_questions = generate_llama_response(user_prompt)\n",
        "print(response_recruiter_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4-Qka5baneG",
        "outputId": "6b81e7e0-cec0-4dc3-e7bc-86ad07e51306"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Sure! Here is a JSON dataset for five senior data scientists with their profiles, including name, position, major, years of experience, technical knowledge, annual salary, and description of current activities:\n",
            "\n",
            "{\n",
            "\"Profiles\": [\n",
            "{\n",
            "\"Name\": \"John Mitchell\",\n",
            "\"Position\": \"Senior Data Scientist\",\n",
            "\"Major\": \"Business\",\n",
            "\"Years of Experience\": 10,\n",
            "\"Technical Knowledge\": [\"Machine Learning\", \"Predictive Modeling\", \"SQL\"],\n",
            "\"Annual Salary\": \"$110,000\",\n",
            "\"Description of Current Activities\": \"Deliver actionable business insights and drive change management initiatives using Python, R, and Tableau.\"\n",
            "},\n",
            "{\n",
            "\"Name\": \"Emily Zhang\",\n",
            "\"Position\": \"Senior Data Scientist\",\n",
            "\"Major\": \"Industrial Engineering\",\n",
            "\"Years of Experience\": 8,\n",
            "\"Technical Knowledge\": [\"Machine Learning\", \"Predictive Modeling\", \"SQL\"],\n",
            "\"Annual Salary\": \"$120,000\",\n",
            "\"Description of Current Activities\": \"Specialize in optimizing industrial processes using machine learning and predictive modeling skills in SQL, Python, R, Tableau, and Power BI.\"\n",
            "},\n",
            "{\n",
            "\"Name\": \"Robert Carter\",\n",
            "\"Position\": \"Senior Data Scientist\",\n",
            "\"Major\": \"Mechanics\",\n",
            "\"Years of Experience\": 7,\n",
            "\"Technical Knowledge\": [\"Machine Learning\", \"Deep Learning\"],\n",
            "\"Annual Salary\": \"$115,000\",\n",
            "\"Description of Current Activities\": \"Apply machine learning and deep learning to predict machinery failures using SQL, Python, R, and Tableau for system optimization and real-time monitoring.\"\n",
            "},\n",
            "{\n",
            "\"Name\": \"Sarah Thompson\",\n",
            "\"Position\": \"Senior Data Scientist\",\n",
            "\"Major\": \"Mathematics\",\n",
            "\"Years of Experience\": 9,\n",
            "\"Technical Knowledge\": [\"Machine Learning\", \"Financial Forecasting\"],\n",
            "\"Annual Salary\": \"$130,000\",\n",
            "\"Description of Current Activities\": \"Focus on complex data modeling and financial forecasting using machine learning, SQL, Python, R, Tableau, and Power BI.\"\n",
            "},\n",
            "{\n",
            "\"Name\": \"David Sanchez\",\n",
            "\"Position\": \"Senior Data Scientist\",\n",
            "\"Major\": \"Economics and Finance\",\n",
            "\"Years of Experience\": 12,\n",
            "\"Technical Knowledge\": [\"Machine Learning\n"
          ]
        }
      ],
      "source": [
        "# Data Scientist Profiles\n",
        "# JSON-based dataset describing senior data scientist profiles\n",
        "user_prompt = '''\n",
        "Create a JSON dataset for the profiles of five senior data scientists with the following details:\n",
        "Name, Position, Major, Years of Experience, Technical Knowledge, Annual Salary, and Description of Current Activities.\n",
        "\n",
        "Profiles:\n",
        "- John Mitchell, Senior Data Scientist – Business Major. With 10 years of experience in business intelligence, I excel in machine learning, predictive modeling, and SQL. Proficient in Python, R, and Tableau, I deliver actionable business insights and drive change management initiatives. Annual Salary: $110,000.\n",
        "- Emily Zhang, Senior Data Scientist – Industrial Engineering Major. With 8 years of experience in operations research and data engineering, I specialize in machine learning and predictive modeling to optimize industrial processes. Skilled in SQL, Python, R, Tableau, and Power BI. Annual Salary: $120,000.\n",
        "- Robert Carter, Senior Data Scientist – Mechanics Major. With 7 years in mechanical engineering, I apply machine learning and deep learning to predict machinery failures. I use SQL, Python, R, and Tableau for system optimization and real-time monitoring. Annual Salary: $115,000.\n",
        "- Sarah Thompson, Senior Data Scientist – Mathematics Major. A data scientist with 9 years in applied mathematics, I focus on complex data modeling and financial forecasting using machine learning, SQL, Python, and R. Proficient in Tableau and Power BI for visualization. Annual Salary: $130,000.\n",
        "- David Sanchez, Senior Data Scientist – Economics and Finance Major. With 12 years of experience, I specialize in financial modeling and risk assessment using machine learning and SQL. Proficient in R, Python, Tableau, and Power BI, I lead data-driven strategies and change management. Annual Salary: $135,000.\n",
        "'''\n",
        "response_profiles = generate_llama_response(user_prompt)\n",
        "print(response_profiles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBsoWp5QaneH"
      },
      "source": [
        "#### **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdYz1gnvhHoI"
      },
      "source": [
        " The goal is to connect with a large language model (LLM) to extract actionable insights on preparing for data science interviews and understanding relevant profiles.\n",
        "\n",
        "To achieve this, I defined clear outcomes by setting specific output expectations. I crafted well-structured and concise prompts for effective LLM interaction. For example, I used prompts like, \"List 10 steps to prepare for a data science interview.\"\n",
        "\n",
        "In formulating these prompts, I incorporated terminology and frameworks pertinent to data science. I included phrases such as \"specific tools and technologies\" and \"attributes of senior data scientists\" to ensure alignment with industry standards.\n",
        "\n",
        "I then compared the LLM's responses against my established objectives, verifying the completeness of the steps provided and assessing their coherence and suitability.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thank you"
      ],
      "metadata": {
        "id": "iSXYmWG4bQ_7"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}